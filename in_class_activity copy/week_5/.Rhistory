# check the result
title_tokens[1:3]
head(stopwords("en"), 15)
# create a new tokens object by removing stopwords from the existing tokens object
title_tokens_nostop <- title_tokens %>%
tokens_remove(stopwords("en"))
# check the result
title_tokens_nostop[1:3]
# Below we create two dfms: one from each of our tokens objects (w/ stopwords and w/o stopwords)
## syntax option 1 (with stopwords)
titles_dfm <- title_tokens %>%
dfm()
## syntax option 2 (without stopwords)
titles_nostop_dfm <- dfm(title_tokens_nostop)
# Print
print(titles_dfm)
print(titles_nostop_dfm)
topfeatures(titles_dfm, 20)
topfeatures(titles_nostop_dfm, 20)
# get top n features
features_titles_dfm <- textstat_frequency(titles_dfm, n = 50)
# sort by reverse frequency order
features_titles_dfm$feature <- with(features_titles_dfm, reorder(feature, -frequency))
ggplot(features_titles_dfm, aes(x = feature, y = frequency)) +
geom_point() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))+
labs(title = "Top words in TC Article Titles (2005-2023)")
# get top n features of no stop dfm
features_titles_nostop_dfm <- textstat_frequency(titles_nostop_dfm, n = 50)
# sort by reverse frequency order
features_titles_nostop_dfm$feature <- with(features_titles_nostop_dfm, reorder(feature, -frequency))
library(ggplot2)
ggplot(features_titles_nostop_dfm, aes(x = feature, y = frequency)) +
geom_point() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Top Words in TC articles without stopwords (2005-2023)")
sorted_features <- features_titles_nostop_dfm %>%
arrange(desc(frequency))%>%
select(feature, frequency, docfreq)%>%
reactable(defaultPageSize = 20,
highlight = TRUE,
outlined = TRUE,
striped = TRUE,
compact = TRUE)
sorted_features
# min count is a big parameter here.
col <- title_tokens %>%
tokens_remove(stopwords("en")) %>%
tokens_select(pattern = "^[A-Z]", valuetype = "regex",
case_insensitive = TRUE, padding = TRUE) %>%
textstat_collocations(min_count = 15, tolower = TRUE)
print(col)
comp_title_toks <- tokens_compound(title_tokens, pattern = col)
head(comp_title_toks)
# load quanteda family of packages
library(quanteda)
library(quanteda.dictionaries)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.textstats)
# load other libraries
library(readtext)
library(spacyr)
library(tidyverse)
# define location of datafile
data_file <- "data/tc_journals.RData"
# load data
load(data_file)
# Create a corpus object called "title_corp_compound_words"
title_corp_compound_words <- corpus(tc_journals, text_field = "article_title")
# create a tokens object without punctuation, separators, and numbers
title_compound_word_tokens <- tokens(title_corp_compound_words, remove_punct = TRUE,
remove_separators = TRUE,
remove_numbers = TRUE)
# removing stopwords
title_compound_word_tokens <- title_compound_word_tokens %>%
tokens_remove(stopwords("en"))
# Making document feature matrices
## Without stopwords
dfm_title_compound_word_tokens <- dfm(title_compound_word_tokens)
collocations_compound <- title_compound_word_tokens %>%
tokens_remove(stopwords("en")) %>%
tokens_select(pattern = "^[A-Z]", valuetype = "regex",
case_insensitive = TRUE, padding = TRUE) %>%
textstat_collocations(min_count = 3, tolower = TRUE)
# Making a corpus
corpus_collocations_compound <- corpus
# making some tokens
tokens_collocations_compound <- tokens(collocations_compound, remove_punct = TRUE,
remove_separators = TRUE,
remove_numbers = TRUE)
# load quanteda family of packages
library(quanteda)
library(quanteda.dictionaries)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.textstats)
# load other libraries
library(readtext)
library(spacyr)
library(tidyverse)
collocations_of_5 <- title_tokens %>%
tokens_remove(stopwords("en")) %>%
tokens_select(pattern = "^[A-Z]", valuetype = "regex",
case_insensitive = TRUE, padding = TRUE) %>%
textstat_collocations(min_count = 5, tolower = TRUE)
head(collocations_of_5)
# making some charts, I guess
collocations_of_5$count <- with(collocations_of_5, reorder(collocation, count))
ggplot(collocations_of_5, aes(x = collocation, y = count)) +
geom_point() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))+
labs(title = "Top collocated words in TC Article Titles (2005-2023)")
View(tc_journals)
# stemming on article abstracts
# load quanteda family of packages
library(quanteda)
library(quanteda.dictionaries)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.textstats)
# load other libraries
library(readtext)
library(spacyr)
library(tidyverse)
library(dplyr)
library(SnowballC)
# define location of datafile
data_file <- "data/tc_journals.RData"
# load data
load(data_file)
# Creating a corpus
abstract_corpus <- corpus(tc_journals, text_field = "abstract")
# Making some tokens
abstract_tokens <- tokens(abstract_corpus, remove_punct = TRUE,
remove_separators = TRUE,
remove_numbers = TRUE)
# removing stopwords, creating a new tokens object
abstract_tokens_no_stop <- abstract_tokens %>%
tokens_remove(stopwords("en"))
# Stemming, using quanteda
abstracts_stemmed <- tokens_wordstem(
abstract_tokens_no_stop,
language = "english"
)
# Making a document-feature matrix
dfm_abstract <- dfm(
abstracts_stemmed,
tolower = TRUE,
remove_padding = FALSE
)
# checking out the top features
topfeatures(dfm_abstract, 100)
# grabbing only the top 25 most frequent tokens
abstracts_top_25_stems <- textstat_frequency(dfm_abstract, n = 25)
# sorting by frequency
abstracts_top_25_stems$feature <- with(
abstracts_top_25_stems,
reorder(feature, -frequency))
# making a graph, hell yea
ggplot(abstracts_top_25_stems, aes(
x = feature, y = frequency)) +
geom_point() +
theme(axis.text = element_text(angle = 45, hjust = 1)) +
labs(title = "Top 25 stems in TC article abstracts 2005-2023")
# load quanteda family of packages
library(quanteda)
library(quanteda.dictionaries)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.textstats)
# load other libraries
library(readtext)
library(spacyr)
library(tidyverse)
# define location of datafile
data_file <- "data/tc_journals.RData"
# load data
load(data_file)
# Create a corpus object called "title_corp_compound_words"
title_corp_compound_words <- corpus(tc_journals, text_field = "article_title")
# create a tokens object without punctuation, separators, and numbers
title_compound_word_tokens <- tokens(title_corp_compound_words, remove_punct = TRUE,
remove_separators = TRUE,
remove_numbers = TRUE)
# removing stopwords
title_compound_word_tokens <- title_compound_word_tokens %>%
tokens_remove(stopwords("en"))
# Making document feature matrices
## Without stopwords
dfm_title_compound_word_tokens <- dfm(title_compound_word_tokens)
collocations_compound <- title_compound_word_tokens %>%
tokens_remove(stopwords("en")) %>%
tokens_select(pattern = "^[A-Z]", valuetype = "regex",
case_insensitive = TRUE, padding = TRUE) %>%
textstat_collocations(min_count = 3, tolower = TRUE)
# Making a corpus
corpus_collocations_compound <- corpus
# making some tokens
tokens_collocations_compound <- tokens(collocations_compound, remove_punct = TRUE,
remove_separators = TRUE,
remove_numbers = TRUE)
# Try three at doing analysis of compound words in R
# load quanteda family of packages
library(quanteda)
library(quanteda.dictionaries)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.textstats)
library(forcats)
# load other libraries
library(readtext)
library(spacyr)
library(tidyverse)
library(dplyr)
library(SnowballC)
# define location of datafile
data_file <- "data/tc_journals.RData"
# load data
load(data_file)
glimpse(tc_journals)
# Making a corpus
title_corpus_compound_words <- corpus(tc_journals, text_field = "article_title")
# Making tokens
title_tokens_compound_words <- tokens(title_corpus_compound_words, remove_punct = TRUE,
remove_separators = TRUE,
remove_numbers = TRUE)
# collocations
col <- title_tokens_compound_words %>%
tokens_remove(stopwords("en")) %>%
tokens_select(pattern = "^[A-Z]", valuetype = "regex",
case_insensitive = TRUE, padding = TRUE) %>%
textstat_collocations(min_count = 15, tolower = TRUE)
# stemming
wordStem(col, language = "en")
# Trying to make a graph, I guess
col %>%
mutate(col$collocation, fct_reorder(col$collocation, col$count))
ggplot(col, aes(x = col$collocation, y = col$count)) +
geom_point() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))+
labs(title = "Top collocations in TC Article Titles (2005-2023)")
# Try three at doing analysis of compound words in R
# load quanteda family of packages
library(quanteda)
library(quanteda.dictionaries)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.textstats)
library(forcats)
# load other libraries
library(readtext)
library(spacyr)
library(tidyverse)
library(dplyr)
library(SnowballC)
# define location of datafile
data_file <- "data/tc_journals.RData"
# load data
load(data_file)
# Making a corpus
title_corpus_compound_words <- corpus(tc_journals, text_field = "article_title")
# Making tokens
title_tokens_compound_words <- tokens(title_corpus_compound_words, remove_punct = TRUE,
remove_separators = TRUE,
remove_numbers = TRUE)
# Making a dfm
dfm_title_tokens
# Try three at doing analysis of compound words in R
# load quanteda family of packages
library(quanteda)
library(quanteda.dictionaries)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.textstats)
library(forcats)
# load other libraries
library(readtext)
library(spacyr)
library(tidyverse)
library(dplyr)
library(SnowballC)
# define location of datafile
data_file <- "data/tc_journals.RData"
# load data
load(data_file)
# Making a corpus
title_corpus_compound_words <- corpus(tc_journals, text_field = "article_title")
# Making tokens
title_tokens_compound_words <- tokens(title_corpus_compound_words, remove_punct = TRUE,
remove_separators = TRUE,
remove_numbers = TRUE)
# Making a dfm
dfm_title_tokens
# Try three at doing analysis of compound words in R
# load quanteda family of packages
library(quanteda)
library(quanteda.dictionaries)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.textstats)
# load other libraries
library(readtext)
library(spacyr)
library(tidyverse)
library(dplyr)
library(SnowballC)
# define location of datafile
data_file <- "data/tc_journals.RData"
# load data
load(data_file)
# Making a corpus
title_corpus <- corpus(tc_journals, text_field = "article_title")
# Making tokens
title_tokens <- tokens(title_corpus, remove_punct = TRUE,
remove_separators = TRUE,
remove_numbers = TRUE)
# create a new tokens object by removing stopwords from the existing tokens object
title_tokens_nostop <- title_tokens %>%
tokens_remove(stopwords("en"))
# stemming, using quanteda. tokens_wordstem needs a tokens object.
titles_stemmed <- tokens_wordstem(
title_tokens_nostop,
language = "english"
)
# making a dfm
dfm_titles <- dfm(
titles_stemmed,
tolower = TRUE,
remove_padding = FALSE
)
# checking out the top features of the dfm
topfeatures(dfm_titles, 100)
# grabbing only the top 25 most frequent tokens
top_25_stems <- textstat_frequency(dfm_titles, n = 25)
# sort by reverse frequency order
top_25_stems$feature <- with(top_100_stems, reorder(feature, -frequency))
# Try three at doing analysis of compound words in R
# load quanteda family of packages
library(quanteda)
library(quanteda.dictionaries)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.textstats)
# load other libraries
library(readtext)
library(spacyr)
library(tidyverse)
library(dplyr)
library(SnowballC)
# define location of datafile
data_file <- "data/tc_journals.RData"
# load data
load(data_file)
# Making a corpus
title_corpus <- corpus(tc_journals, text_field = "article_title")
# Making tokens
title_tokens <- tokens(title_corpus, remove_punct = TRUE,
remove_separators = TRUE,
remove_numbers = TRUE)
# create a new tokens object by removing stopwords from the existing tokens object
title_tokens_nostop <- title_tokens %>%
tokens_remove(stopwords("en"))
# stemming, using quanteda. tokens_wordstem needs a tokens object.
titles_stemmed <- tokens_wordstem(
title_tokens_nostop,
language = "english"
)
# making a dfm
dfm_titles <- dfm(
titles_stemmed,
tolower = TRUE,
remove_padding = FALSE
)
# checking out the top features of the dfm
topfeatures(dfm_titles, 100)
# grabbing only the top 25 most frequent tokens
top_25_stems <- textstat_frequency(dfm_titles, n = 25)
# sort by reverse frequency order
top_25_stems$feature <- with(top_25_stems, reorder(feature, -frequency))
# making a graph, hell yeah
ggplot(top_25_stems, aes(
x = feature, y = frequency)) +
geom_point() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Top 25 stems in TC article titles 2005-2023")
# stemming on article abstracts
# load quanteda family of packages
library(quanteda)
library(quanteda.dictionaries)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.textstats)
# load other libraries
library(readtext)
library(spacyr)
library(tidyverse)
library(dplyr)
library(SnowballC)
# define location of datafile
data_file <- "data/tc_journals.RData"
# load data
load(data_file)
# Creating a corpus
abstract_corpus <- corpus(tc_journals, text_field = "abstract")
# Making some tokens
abstract_tokens <- tokens(abstract_corpus, remove_punct = TRUE,
remove_separators = TRUE,
remove_numbers = TRUE)
# removing stopwords, creating a new tokens object
abstract_tokens_no_stop <- abstract_tokens %>%
tokens_remove(stopwords("en"))
# Stemming, using quanteda
abstracts_stemmed <- tokens_wordstem(
abstract_tokens_no_stop,
language = "english"
)
# Making a document-feature matrix
dfm_abstract <- dfm(
abstracts_stemmed,
tolower = TRUE,
remove_padding = FALSE
)
# checking out the top features
topfeatures(dfm_abstract, 100)
# grabbing only the top 25 most frequent tokens
abstracts_top_25_stems <- textstat_frequency(dfm_abstract, n = 25)
# sorting by frequency
abstracts_top_25_stems$feature <- with(
abstracts_top_25_stems,
reorder(feature, -frequency))
# making a graph, hell yea
ggplot(abstracts_top_25_stems, aes(
x = feature, y = frequency)) +
geom_point() +
theme(axis.text = element_text(angle = 45, hjust = 1)) +
labs(title = "Top 25 stems in TC article abstracts 2005-2023")
# Try three at doing analysis of compound words in R
# load quanteda family of packages
library(quanteda)
library(quanteda.dictionaries)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.textstats)
library(forcats)
# load other libraries
library(readtext)
library(spacyr)
library(tidyverse)
library(dplyr)
library(SnowballC)
# define location of datafile
data_file <- "data/tc_journals.RData"
# load data
load(data_file)
# Making a corpus
title_corpus_compound_words <- corpus(tc_journals, text_field = "article_title")
# Making tokens
title_tokens_compound_words <- tokens(title_corpus_compound_words, remove_punct = TRUE,
remove_separators = TRUE,
remove_numbers = TRUE)
# Making a dfm
dfm_title_tokens
# Try three at doing analysis of compound words in R
# load quanteda family of packages
library(quanteda)
library(quanteda.dictionaries)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.textstats)
# load other libraries
library(readtext)
library(spacyr)
library(tidyverse)
library(dplyr)
library(SnowballC)
# define location of datafile
data_file <- "data/tc_journals.RData"
# load data
load(data_file)
# Making a corpus
title_corpus <- corpus(tc_journals, text_field = "article_title")
# Making tokens
title_tokens <- tokens(title_corpus, remove_punct = TRUE,
remove_separators = TRUE,
remove_numbers = TRUE)
# create a new tokens object by removing stopwords from the existing tokens object
title_tokens_nostop <- title_tokens %>%
tokens_remove(stopwords("en"))
# stemming, using quanteda. tokens_wordstem needs a tokens object.
titles_stemmed <- tokens_wordstem(
title_tokens_nostop,
language = "english"
)
# making a dfm
dfm_titles <- dfm(
titles_stemmed,
tolower = TRUE,
remove_padding = FALSE
)
# checking out the top features of the dfm
topfeatures(dfm_titles, 100)
# grabbing only the top 25 most frequent tokens
top_25_stems <- textstat_frequency(dfm_titles, n = 25)
# sort by reverse frequency order
top_25_stems$feature <- with(top_25_stems, reorder(feature, -frequency))
# making a graph, hell yeah
ggplot(top_25_stems, aes(
x = feature, y = frequency)) +
geom_point() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Top 25 stems in TC article titles 2005-2023")
